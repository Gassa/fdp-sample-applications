akka {                                                                         
  loglevel = INFO                                                             
  log-config-on-start = on                                                     
  loggers = ["akka.event.slf4j.Slf4jLogger"]                                   
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"                       
  event-handlers = ["akka.event.slf4j.Slf4jEventHandler"]                      
}

dcos {

  kafka {
    ## bootstrap servers for Kafka
    brokers = "localhost:9092"
    brokers = ${?KAFKA_BROKERS}

    ## consumer group
    group = "group-dsl"
    group = ${?KAFKA_GROUP_DSL}

    ## the source topic - processing starts with
    ## data in this topic (to be loaded by ingestion)
    fromtopic = "server-log-dsl"
    fromtopic = ${?KAFKA_FROM_TOPIC_DSL}

    ## processed records goes here in json of LogRecord
    totopic = "processed-log"
    totopic = ${?KAFKA_TO_TOPIC_DSL}

    ## this gets the avro serialized data from totopic for processing by Kafka Connect
    ## HDFS sink connector
    avrotopic = "avro-topic"
    avrotopic = ${?KAFKA_AVRO_TOPIC_DSL}

    ## summary access information gets pushed here
    summaryaccesstopic = "summary-access-log"
    summaryaccesstopic = ${?KAFKA_SUMMARY_ACCESS_TOPIC_DSL}

    ## windowed summary access information gets pushed here
    windowedsummaryaccesstopic = "windowed-summary-access-log"
    windowedsummaryaccesstopic = ${?KAFKA_WINDOWED_SUMMARY_ACCESS_TOPIC_DSL}

    ## summary payload information gets pushed here
    summarypayloadtopic = "summary-payload-log"
    summarypayloadtopic = ${?KAFKA_SUMMARY_PAYLOAD_TOPIC_DSL}

    ## windowed summary payload information gets pushed here
    windowedsummarypayloadtopic = "windowed-summary-payload-log"
    windowedsummarypayloadtopic = ${?KAFKA_WINDOWED_SUMMARY_PAYLOAD_TOPIC_DSL}

    ## error topic for the initial processing
    errortopic = "logerr-dsl"
    errortopic = ${?KAFKA_ERROR_TOPIC_DSL}

    zookeeper = "localhost:2181"
    zookeeper = ${?ZOOKEEPER_URL}

    schemaregistryurl = "http://localhost:8081"
    schemaregistryurl = ${?SCHEMA_REGISTRY_URL}

    ## folder where state stores are created by Kafka Streams
    statestoredir = "/tmp/kafka-streams"
    statestoredir = ${?STATESTOREDIR}

    ## settings for data ingestion
    loader {
      sourcetopic = ${dcos.kafka.fromtopic}
      sourcetopic = ${?KAFKA_FROM_TOPIC_DSL}

      directorytowatch = "/Users/debasishghosh/t"
      directorytowatch = ${?DIRECTORY_TO_WATCH}

      pollinterval = 1 second
    }
  }

  # http endpoints of the weblog microservice
  http {
    # The port the dashboard listens on
    port = 7070
    port = ${?PORT_DSL}

    # The interface the dashboard listens on
    interface = "localhost"
    interface = ${?INTERFACE_DSL}
  }
}

