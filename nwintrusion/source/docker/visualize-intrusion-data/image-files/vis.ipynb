{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer, KafkaError\n",
    "from lightning import Lightning\n",
    "from numpy import random\n",
    "from numpy import concatenate\n",
    "from numpy import array\n",
    "from numpy import empty\n",
    "from numpy import append\n",
    "from numpy import around\n",
    "from numpy import log\n",
    "from numpy import fabs\n",
    "import time\n",
    "\n",
    "outliers_x,outliers_y,data_points_x,data_points_y,cl_x,cl_y = ([],[],[],[],[],[])\n",
    "x_n,y_n=(array([], dtype=float), array([], dtype=float))\n",
    "centroid_data_x = centroid_data_y = {}\n",
    "\n",
    "centroid_records_seen = 0\n",
    "total_records_seen = 0\n",
    "\n",
    "first_centroids_record_passed = False\n",
    "\n",
    "# max number of points per cluster to show (we use this for practical reasons)\n",
    "MAX_DATA_POINTS_PER_CLUSTER = 25\n",
    "\n",
    "MAX_OUTLIERS_TO_SHOW = 25\n",
    "\n",
    "# blue color for centroids\n",
    "centroid_color = [0, 0, 255]\n",
    "\n",
    "# green color for points\n",
    "point_color = [0, 255, 0]\n",
    "\n",
    "# red color for outlier points\n",
    "outlier_color = [255, 0, 0]\n",
    "\n",
    "cluster_topic=\"nwcls\"\n",
    "brokers=\"broker.kafka.l4lb.thisdcos.directory:9092\"\n",
    "\n",
    "lgn = Lightning()\n",
    "\n",
    "description = '''\n",
    "<div style=\"text-align:center\">\n",
    "<span style=\"color: black; font-weight: bold;\">Network Intrusion App - Visualization </span> </br>\n",
    "<span style=\"color: orange; font-weight: bold;\">FDP Platform - Lightbend </span>\n",
    "</div>'''\n",
    "\n",
    "viz=None\n",
    "\n",
    "def str_to_bool(s):\n",
    "    if s == 'True':\n",
    "         return True\n",
    "    elif s == 'False':\n",
    "         return False\n",
    "    else:\n",
    "         raise ValueError\n",
    "        \n",
    "def parse_msg (msg):\n",
    "  global viz\n",
    "  global data_points_x,data_points_y,cl_x,cl_y\n",
    "  global x_n\n",
    "  global y_n\n",
    "  global centroid_data_x, centroid_data_y\n",
    "  global centroid_records_seen\n",
    "  global total_records_seen\n",
    "  global first_centroids_record_passed\n",
    "\n",
    "  isOutlier = False\n",
    "  total_records_seen = total_records_seen + 1\n",
    "\n",
    "  data_points_x,data_points_y = ([],[])\n",
    "\n",
    "  if  msg.find(\"Centroids:/\") >= 0 :\n",
    "    #print(msg)\n",
    "    centroid_records_seen =  centroid_records_seen + 1\n",
    "    splitted=msg.strip(\"Centroids:/\").split(\"/\")\n",
    "    data_size=len(splitted)\n",
    "\n",
    "    # new clusters arrived remove old data\n",
    "    cl_x,cl_y = ([],[])\n",
    "    centroid_data_x = centroid_data_y = {}\n",
    "    x_n,y_n = (array([], dtype=float), array([], dtype=float))\n",
    "\n",
    "    for cluster_center_point in splitted:\n",
    "      if not first_centroids_record_passed: # clean old data for all cases but the first centroid record\n",
    "        centroid_data_x[cluster_center_point] = []\n",
    "        centroid_data_y[cluster_center_point] = []\n",
    "      else : # It might be the case that we received data first\n",
    "        centroid_data_x.setdefault(cluster_center_point, [])\n",
    "        centroid_data_y.setdefault(cluster_center_point, [])\n",
    "      pair = cluster_center_point.split(\",\")\n",
    "      cl_x.append(float(pair[0]))\n",
    "      cl_y.append(float(pair[1]))\n",
    "    \n",
    "    if not first_centroids_record_passed:\n",
    "        first_centroids_record_passed = True \n",
    "        \n",
    "  else :\n",
    "    splitted = msg.split(\",\")\n",
    "    cluster_center_point = splitted[1].strip(\"[\") + \",\" + splitted[2].strip(\"]\")\n",
    "    x_1 = splitted[-2] # new point x coordinate \n",
    "    y_1 = splitted[-1] # new point y coordinate\n",
    "    isOutlier = str_to_bool(splitted[5].capitalize())\n",
    "    \n",
    "    if isOutlier:\n",
    "        points_for_update_x = outliers_x\n",
    "        points_for_update_y = outliers_y\n",
    "    else: \n",
    "        if first_centroids_record_passed: # we might be the first to update the map\n",
    "            centroid_data_x.setdefault(cluster_center_point, [])\n",
    "            centroid_data_y.setdefault(cluster_center_point, [])\n",
    "        points_for_update_x = centroid_data_x[cluster_center_point]\n",
    "        points_for_update_y = centroid_data_y[cluster_center_point]\n",
    "\n",
    "    if len(points_for_update_x) <= MAX_DATA_POINTS_PER_CLUSTER:\n",
    "        points_for_update_x.append(float(x_1))\n",
    "        points_for_update_y.append(float(y_1))\n",
    "    else:\n",
    "        record_pointer = (total_records_seen - centroid_records_seen) % MAX_DATA_POINTS_PER_CLUSTER\n",
    "        points_for_update_x[record_pointer] = float(x_1)\n",
    "        points_for_update_y[record_pointer] = float(y_1)\n",
    "      \n",
    "    \n",
    "  for value in centroid_data_x.values():\n",
    "    data_points_x = data_points_x + value\n",
    "    \n",
    "  for value in centroid_data_y.values():\n",
    "    data_points_y = data_points_y + value\n",
    "\n",
    "  cl_n_x = array(cl_x)\n",
    "  cl_n_y = array(cl_y)\n",
    "\n",
    "  data_n_x = array(data_points_x)\n",
    "  data_n_y = array(data_points_y)\n",
    "    \n",
    "  outliers_n_x = array(outliers_x)\n",
    "  outliers_n_y = array(outliers_y)\n",
    "\n",
    "  x_n = concatenate((cl_n_x, data_n_x))\n",
    "  x_n = concatenate((x_n, outliers_n_x))\n",
    "  y_n = concatenate((cl_n_y, data_n_y))\n",
    "  y_n = concatenate((y_n, outliers_n_y))\n",
    "\n",
    "  # start visualization here, it does not work if we start with empty data\n",
    "  if viz == None:\n",
    "    viz = lgn.scatterstreaming(x_n, y_n, description = description)\n",
    "    vis_link = viz.get_permalink()\n",
    "    public_ip_list = !curl http://169.254.169.254/latest/meta-data/public-ipv4\n",
    "    host_public_ip=public_ip_list.l[-1]\n",
    "    print(\"Click here to view streaming k-means: \" + vis_link.replace('localhost', host_public_ip))\n",
    "    \n",
    "  data_colors = empty((len(data_points_x),),dtype=object)\n",
    "  cluster_center_colors = empty((len(cl_x),),dtype=object)\n",
    "  outliers_colors = empty((len(outliers_x),),dtype=object)\n",
    "\n",
    "  data_colors.fill(point_color)\n",
    "  cluster_center_colors.fill(centroid_color)\n",
    "  outliers_colors.fill(outlier_color)\n",
    "\n",
    "  color_list = concatenate((cluster_center_colors, data_colors))\n",
    "  color_list = concatenate((color_list, outliers_colors)).tolist()\n",
    "  viz.append(x_n, y_n, color = color_list)\n",
    "  return\n",
    "\n",
    "# Make sure we restart when we re-run the code for the same group id\n",
    "def on_assign (c, ps):\n",
    "    for p in ps:\n",
    "        print(\"Topic info...\" + p)\n",
    "        p.offset=-2\n",
    "        c.assign(ps)\n",
    "\n",
    "# Create a consumer\n",
    "rand_suffix = str(int(round(time.time() * 1000)))\n",
    "c = Consumer({'bootstrap.servers': brokers, 'group.id': 'consume-nwcin-' + rand_suffix,\n",
    "                'default.topic.config': {'auto.offset.reset': 'earliest'}})\n",
    "\n",
    "# subscribe to the cluster topic. We always start from the beginning\n",
    "# even if we re-use the same group.id\n",
    "c.subscribe([cluster_topic])\n",
    "\n",
    "# Loop until we have an error, blocks if no messages are left\n",
    "counter=0\n",
    "running = True\n",
    "while running:\n",
    "\n",
    "    start = time.time()\n",
    "    msg = c.poll()\n",
    "\n",
    "    if not msg.error():\n",
    "        #print('Received message: %s' % msg.value().decode('utf-8'))\n",
    "        counter = counter  + 1\n",
    "        #if counter % 10000 == 0:\n",
    "        #    print(\"Records processed...\" + str(counter))\n",
    "        parse_msg(msg.value().decode('utf-8'))\n",
    "    elif msg.error().code() != KafkaError._PARTITION_EOF:\n",
    "        print(msg.error())\n",
    "        running = False\n",
    "c.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
